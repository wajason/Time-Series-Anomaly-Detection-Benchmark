{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Downloading Packages***"
      ],
      "metadata": {
        "id": "4LcYAS8OUIr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q numpy pandas scikit-learn matplotlib tqdm"
      ],
      "metadata": {
        "id": "Z85uFL7xXjTt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Import packages***"
      ],
      "metadata": {
        "id": "rDu8_FjQXoms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_9AtNFhXk-h",
        "outputId": "c28e91c1-dc4b-4561-fa20-5a1d748de68f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Downloading TSB-UAD***"
      ],
      "metadata": {
        "id": "BHag69CMZ2GM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_ROOT = Path(\"/content/data\")\n",
        "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "ZIP_PATH = DATA_ROOT / \"TSB-UAD-Public.zip\"\n",
        "\n",
        "if not ZIP_PATH.exists():\n",
        "    print(\"Downloading TSB-UAD-Public.zip ...\")\n",
        "    !wget -q -O /content/data/TSB-UAD-Public.zip \"https://www.thedatum.org/datasets/TSB-UAD-Public.zip\"\n",
        "    print(\"Download done.\")\n",
        "\n",
        "EXTRACTED_FLAG = DATA_ROOT / \"tsb_extracted.flag\"\n",
        "if not EXTRACTED_FLAG.exists():\n",
        "    print(\"Unzipping ...\")\n",
        "    !unzip -q /content/data/TSB-UAD-Public.zip -d /content/data\n",
        "    EXTRACTED_FLAG.touch()\n",
        "    print(\"Unzip done.\")\n",
        "\n",
        "all_out_files = list(DATA_ROOT.rglob(\"*.out\"))\n",
        "print(\"Found\", len(all_out_files), \".out files\")\n",
        "\n",
        "sample_path = sorted(all_out_files)[0]\n",
        "print(\"Using sample file:\", sample_path)\n",
        "\n",
        "\n",
        "def load_tsb_out_file(path: Path):\n",
        "    \"\"\"\n",
        "    TSB-UAD .out 格式：\n",
        "    第 1 欄：time series 值\n",
        "    第 2 欄：label (0: normal, 1: anomaly)\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(path, header=None)\n",
        "    values = df.iloc[:, 0].astype(float).to_numpy()\n",
        "    labels = df.iloc[:, 1].astype(int).to_numpy()\n",
        "    return values, labels\n",
        "\n",
        "\n",
        "values, labels = load_tsb_out_file(sample_path)\n",
        "print(\"Series length:\", len(values))\n",
        "print(\"Anomaly ratio:\", labels.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyW364eNXq5g",
        "outputId": "b3e870f4-bbf3-4897-94f8-81504fb4e451"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading TSB-UAD-Public.zip ...\n",
            "Download done.\n",
            "Unzipping ...\n",
            "Unzip done.\n",
            "Found 2062 .out files\n",
            "Using sample file: /content/data/TSB-UAD-Public/Daphnet/S01R02E0.test.csv@1.out\n",
            "Series length: 28800\n",
            "Anomaly ratio: 0.05371527777777778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Sliding Windows***"
      ],
      "metadata": {
        "id": "BWvzGrolZ_SC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def sliding_windows(x: np.ndarray, window: int, step: int = 1):\n",
        "    x = np.asarray(x)\n",
        "    n = len(x)\n",
        "    idx = []\n",
        "    for i in range(0, n - window + 1, step):\n",
        "        idx.append(x[i:i+window])\n",
        "    return np.stack(idx, axis=0)"
      ],
      "metadata": {
        "id": "mfRl6wWCZ1T3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Map window scores to points***"
      ],
      "metadata": {
        "id": "hqCFmPkuaG8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_window_scores_to_points(window_scores: np.ndarray, series_len: int, window: int, step: int = 1):\n",
        "    window_scores = np.asarray(window_scores)\n",
        "    n_windows = window_scores.shape[0]\n",
        "    point_scores = np.zeros(series_len, dtype=float)\n",
        "    counts = np.zeros(series_len, dtype=int)\n",
        "\n",
        "    w = window\n",
        "    idx = 0\n",
        "    for start in range(0, series_len - w + 1, step):\n",
        "        s = start\n",
        "        e = start + w\n",
        "        point_scores[s:e] += window_scores[idx]\n",
        "        counts[s:e] += 1\n",
        "        idx += 1\n",
        "\n",
        "    counts[counts == 0] = 1\n",
        "    point_scores /= counts\n",
        "    return point_scores\n"
      ],
      "metadata": {
        "id": "dicFkNiGaCGY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Evaluate***"
      ],
      "metadata": {
        "id": "xNe_0N-KaMYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(scores: np.ndarray, labels: np.ndarray):\n",
        "    scores = np.asarray(scores).ravel()\n",
        "    labels = np.asarray(labels).ravel().astype(int)\n",
        "    if len(np.unique(labels)) < 2:\n",
        "        return {\"auc_roc\": np.nan, \"auc_pr\": np.nan}\n",
        "    auc_roc = roc_auc_score(labels, scores)\n",
        "    auc_pr = average_precision_score(labels, scores)\n",
        "    return {\"auc_roc\": auc_roc, \"auc_pr\": auc_pr}"
      ],
      "metadata": {
        "id": "4eEY6fjhaKzm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Plot series and scores***"
      ],
      "metadata": {
        "id": "GgZF06RFaPkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def plot_series_and_scores(values, labels, scores_dict, max_points=2000, title=\"\"):\n",
        "    \"\"\"\n",
        "    values: 1D series\n",
        "    labels: 0/1\n",
        "    scores_dict: {name: score_array}\n",
        "    \"\"\"\n",
        "    n = len(values)\n",
        "    if n > max_points:\n",
        "        # 太長就截前面 max_points 畫\n",
        "        values = values[:max_points]\n",
        "        labels = labels[:max_points]\n",
        "        scores_dict = {k: v[:max_points] for k, v in scores_dict.items()}\n",
        "        n = max_points\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n",
        "\n",
        "    ax1.plot(values, label=\"value\")\n",
        "    ax1.scatter(np.where(labels == 1)[0], values[labels == 1],\n",
        "                marker=\"x\", s=40, label=\"anomaly (label)\")\n",
        "    ax1.set_ylabel(\"value\")\n",
        "    ax1.set_title(title)\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "\n",
        "    for name, score in scores_dict.items():\n",
        "        score_norm = (score - score.min()) / (score.max() - score.min() + 1e-8)\n",
        "        ax2.plot(score_norm, label=name)\n",
        "    ax2.set_ylabel(\"normalized score\")\n",
        "    ax2.legend(loc=\"upper right\")\n",
        "\n",
        "    plt.xlabel(\"time\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "w2G7pLRVaMG2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Series2Graph***"
      ],
      "metadata": {
        "id": "qZulAwI-aX2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "def map_window_scores_to_points(window_scores: np.ndarray, series_len: int, window: int, step: int = 1):\n",
        "    window_scores = np.asarray(window_scores)\n",
        "    point_scores = np.zeros(series_len, dtype=float)\n",
        "    counts = np.zeros(series_len, dtype=int)\n",
        "\n",
        "    idx = 0\n",
        "    for start in range(0, series_len - window + 1, step):\n",
        "        s = start\n",
        "        e = start + window\n",
        "        point_scores[s:e] += window_scores[idx]\n",
        "        counts[s:e] += 1\n",
        "        idx += 1\n",
        "\n",
        "    counts[counts == 0] = 1\n",
        "    point_scores /= counts\n",
        "    return point_scores\n",
        "\n",
        "def s2g_subsequence_embedding(values: np.ndarray, ell: int, lam: int):\n",
        "\n",
        "    vals = np.asarray(values, dtype=float)\n",
        "    n = len(vals)\n",
        "    if n < ell or lam <= 0 or lam >= ell:\n",
        "        return np.zeros((max(1, n - ell + 1), 2), dtype=float)\n",
        "\n",
        "    subseqs = sliding_windows(vals, window=ell, step=1)\n",
        "    L_proj = ell - lam + 1\n",
        "    proj_list = []\n",
        "    for s in subseqs:\n",
        "        sm = np.convolve(s, np.ones(lam, dtype=float), mode=\"valid\")\n",
        "\n",
        "        proj_list.append(sm)\n",
        "    Proj = np.stack(proj_list, axis=0)\n",
        "    pca = PCA(n_components=3)\n",
        "    Proj_r = pca.fit_transform(Proj)\n",
        "    ry = Proj_r[:, 1]\n",
        "    rz = Proj_r[:, 2]\n",
        "    SP_proj = np.stack([ry, rz], axis=1)\n",
        "\n",
        "    return SP_proj\n",
        "\n",
        "def s2g_node_creation(SP_proj: np.ndarray,\n",
        "                      psi_list: np.ndarray,\n",
        "                      num_bins: int = 30,\n",
        "                      min_count: int = 3):\n",
        "\n",
        "    points = np.asarray(SP_proj, dtype=float)\n",
        "    num_points = points.shape[0]\n",
        "    if num_points == 0:\n",
        "        return [], {}\n",
        "\n",
        "    nodes = []\n",
        "    node_id = 0\n",
        "\n",
        "    psi_to_nodes = {}\n",
        "\n",
        "    for psi_idx, psi in enumerate(psi_list):\n",
        "        cos_p = np.cos(psi)\n",
        "        sin_p = np.sin(psi)\n",
        "\n",
        "        s = points[:, 0] * cos_p + points[:, 1] * sin_p\n",
        "        s_min, s_max = s.min(), s.max()\n",
        "        if s_max == s_min:\n",
        "            continue\n",
        "\n",
        "        hist, bin_edges = np.histogram(s, bins=num_bins, range=(s_min, s_max))\n",
        "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2.0\n",
        "        for b in range(1, num_bins - 1):\n",
        "            c = hist[b]\n",
        "            if c < min_count:\n",
        "                continue\n",
        "            if c >= hist[b - 1] and c >= hist[b + 1]:\n",
        "                node = {\n",
        "                    \"id\": node_id,\n",
        "                    \"psi_idx\": psi_idx,\n",
        "                    \"s_coord\": float(bin_centers[b]),\n",
        "                }\n",
        "                nodes.append(node)\n",
        "                psi_to_nodes.setdefault(psi_idx, []).append(node_id)\n",
        "                node_id += 1\n",
        "\n",
        "    node_meta = {\n",
        "        \"psi_list\": psi_list,\n",
        "        \"psi_to_nodes\": psi_to_nodes,\n",
        "        \"nodes\": nodes,\n",
        "    }\n",
        "    return nodes, node_meta\n",
        "\n",
        "def s2g_edge_creation(SP_proj: np.ndarray, nodes, node_meta):\n",
        "\n",
        "    points = np.asarray(SP_proj, dtype=float)\n",
        "    num_points = points.shape[0]\n",
        "    if num_points == 0 or len(nodes) == 0:\n",
        "        return np.zeros(num_points, dtype=int), {}, {}\n",
        "\n",
        "    psi_list = node_meta[\"psi_list\"]\n",
        "    psi_to_nodes = node_meta[\"psi_to_nodes\"]\n",
        "\n",
        "    node_info = {}\n",
        "    for nd in nodes:\n",
        "        node_info[nd[\"id\"]] = (nd[\"psi_idx\"], nd[\"s_coord\"])\n",
        "    thetas = np.arctan2(points[:, 1], points[:, 0])\n",
        "    thetas = (thetas + 2.0 * np.pi) % (2.0 * np.pi)\n",
        "\n",
        "    node_seq = np.zeros(num_points, dtype=int)\n",
        "\n",
        "    for i in range(num_points):\n",
        "        theta = thetas[i]\n",
        "        x_y, x_z = points[i, 0], points[i, 1]\n",
        "\n",
        "        delta = np.abs(psi_list - theta)\n",
        "        delta = np.minimum(delta, 2.0 * np.pi - delta)\n",
        "        psi_idx = int(np.argmin(delta))\n",
        "\n",
        "        if psi_idx not in psi_to_nodes or len(psi_to_nodes[psi_idx]) == 0:\n",
        "            found = False\n",
        "            for shift in range(1, len(psi_list)):\n",
        "                left = (psi_idx - shift) % len(psi_list)\n",
        "                right = (psi_idx + shift) % len(psi_list)\n",
        "                if left in psi_to_nodes and len(psi_to_nodes[left]) > 0:\n",
        "                    psi_idx = left\n",
        "                    found = True\n",
        "                    break\n",
        "                if right in psi_to_nodes and len(psi_to_nodes[right]) > 0:\n",
        "                    psi_idx = right\n",
        "                    found = True\n",
        "                    break\n",
        "            if not found:\n",
        "                node_seq[i] = 0\n",
        "                continue\n",
        "        cos_p = np.cos(psi_idx * (2.0 * np.pi / len(psi_list)))\n",
        "        sin_p = np.sin(psi_idx * (2.0 * np.pi / len(psi_list)))\n",
        "        s_x = x_y * cos_p + x_z * sin_p\n",
        "\n",
        "        best_node = None\n",
        "        best_dist = np.inf\n",
        "        for nid in psi_to_nodes[psi_idx]:\n",
        "            _, s_coord = node_info[nid]\n",
        "            d = abs(s_x - s_coord)\n",
        "            if d < best_dist:\n",
        "                best_dist = d\n",
        "                best_node = nid\n",
        "\n",
        "        node_seq[i] = best_node if best_node is not None else 0\n",
        "\n",
        "    edge_w = {}\n",
        "    adj = {}\n",
        "\n",
        "    for i in range(num_points - 1):\n",
        "        a = int(node_seq[i])\n",
        "        b = int(node_seq[i + 1])\n",
        "        edge_w[(a, b)] = edge_w.get((a, b), 0) + 1\n",
        "\n",
        "        adj.setdefault(a, set()).add(b)\n",
        "        adj.setdefault(b, set()).add(a)\n",
        "\n",
        "    degree = {nid: len(neis) for nid, neis in adj.items()}\n",
        "\n",
        "    return node_seq, edge_w, degree\n",
        "\n",
        "def s2g_subsequence_scoring(node_seq: np.ndarray,\n",
        "                            edge_w: dict,\n",
        "                            degree: dict,\n",
        "                            series_len: int,\n",
        "                            ell: int):\n",
        "    node_seq = np.asarray(node_seq, dtype=int)\n",
        "    L = len(node_seq)\n",
        "\n",
        "    if L <= 1 or series_len <= 0:\n",
        "        return np.zeros(series_len, dtype=float)\n",
        "\n",
        "    edge_scores = np.zeros(L - 1, dtype=float)\n",
        "    for j in range(L - 1):\n",
        "        a = int(node_seq[j])\n",
        "        b = int(node_seq[j + 1])\n",
        "        w_ab = edge_w.get((a, b), 0)\n",
        "        deg_a = degree.get(a, 0)\n",
        "        edge_scores[j] = w_ab * max(deg_a - 1, 0)\n",
        "    point_scores = np.zeros(series_len, dtype=float)\n",
        "    counts = np.zeros(series_len, dtype=int)\n",
        "\n",
        "    for j in range(L - 1):\n",
        "        t_center = j + ell // 2\n",
        "        if 0 <= t_center < series_len:\n",
        "            point_scores[t_center] += edge_scores[j]\n",
        "            counts[t_center] += 1\n",
        "\n",
        "    counts[counts == 0] = 1\n",
        "    point_scores /= counts\n",
        "\n",
        "    anom_scores = -point_scores\n",
        "    return anom_scores\n",
        "\n",
        "\n",
        "def series2graph_detector(values: np.ndarray,\n",
        "                          window: int = 64,\n",
        "                          step: int = 1,\n",
        "                          num_angles: int = 64,\n",
        "                          num_bins: int = 30):\n",
        "    vals = np.asarray(values, dtype=float)\n",
        "    n = len(vals)\n",
        "    if n < window + 2:\n",
        "        return np.zeros(n, dtype=float)\n",
        "\n",
        "    lam = max(1, window // 3)\n",
        "    SP_proj = s2g_subsequence_embedding(vals, ell=window, lam=lam)\n",
        "\n",
        "    psi_list = np.linspace(0.0, 2.0 * np.pi, num=num_angles, endpoint=False)\n",
        "    nodes, node_meta = s2g_node_creation(SP_proj, psi_list=psi_list,\n",
        "                                         num_bins=num_bins, min_count=3)\n",
        "\n",
        "    if len(nodes) == 0:\n",
        "        return np.zeros(n, dtype=float)\n",
        "\n",
        "    node_seq, edge_w, degree = s2g_edge_creation(SP_proj, nodes=nodes, node_meta=node_meta)\n",
        "##############################  TODO:  #############################################\n",
        "#Complete the Series2Graph\n",
        "    point_scores = s2g_subsequence_scoring(\n",
        "            node_seq=node_seq,\n",
        "            edge_w=edge_w,\n",
        "            degree=degree,\n",
        "            series_len=n,       # 對應總長度 n\n",
        "            ell=window         # 對應視窗大小 window\n",
        "        )\n",
        "########################################################################################\n",
        "    scaler = MinMaxScaler()\n",
        "    point_scores = scaler.fit_transform(point_scores.reshape(-1, 1)).ravel()\n",
        "    return point_scores\n",
        "\n"
      ],
      "metadata": {
        "id": "_fS-cVCSaR3T"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***AutoEncoder***"
      ],
      "metadata": {
        "id": "2q_-fwv5ai9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WindowDataset(Dataset):\n",
        "    def __init__(self, values: np.ndarray, window: int, step: int = 1):\n",
        "        self.values = np.asarray(values, dtype=float)\n",
        "        self.window = window\n",
        "        self.step = step\n",
        "        self.windows = sliding_windows(self.values, window=window, step=step)\n",
        "        self.windows = self.windows.astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.windows.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.windows[idx]\n",
        "        return torch.from_numpy(x)\n",
        "\n",
        "\n",
        "class AE_MLP(nn.Module):\n",
        "    def __init__(self, window: int, hidden_dim: int = 64, bottleneck: int = 16):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(window, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, bottleneck),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(bottleneck, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, window)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        out = self.decoder(z)\n",
        "        return out\n",
        "\n",
        "\n",
        "def autoencoder_detector(values: np.ndarray, window: int = 64, step: int = 1,\n",
        "                         train_ratio: float = 0.2, epochs: int = 30, batch_size: int = 128,\n",
        "                         lr: float = 1e-3):\n",
        "\n",
        "    vals = np.asarray(values, dtype=float)\n",
        "    dataset_all = WindowDataset(vals, window=window, step=step)\n",
        "    n_windows = len(dataset_all)\n",
        "    n_train = max(10, int(train_ratio * n_windows))\n",
        "    train_dataset = torch.utils.data.Subset(dataset_all, range(n_train))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "    model = AE_MLP(window=window).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0.0\n",
        "        for batch in train_loader:\n",
        "            batch = batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            recon = model(batch)\n",
        "            loss = criterion(recon, batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item() * batch.size(0)\n",
        "        epoch_loss /= len(train_loader.dataset)\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f\"[AE] Epoch {epoch+1}/{epochs} - loss={epoch_loss:.6f}\")\n",
        "\n",
        "    model.eval()\n",
        "    all_loader = DataLoader(dataset_all, batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "    window_scores = []\n",
        "    with torch.no_grad():\n",
        "        for batch in all_loader:\n",
        "            batch = batch.to(device)\n",
        "            recon = model(batch)\n",
        "            mse = torch.mean((recon - batch) ** 2, dim=1)\n",
        "            window_scores.append(mse.cpu().numpy())\n",
        "    window_scores = np.concatenate(window_scores, axis=0)\n",
        "    point_scores = map_window_scores_to_points(window_scores, len(vals), window=window, step=step)\n",
        "    scaler = MinMaxScaler()\n",
        "    point_scores = scaler.fit_transform(point_scores.reshape(-1, 1)).ravel()\n",
        "    return point_scores"
      ],
      "metadata": {
        "id": "A14IkI6zab5G"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***LSTM-AD***"
      ],
      "metadata": {
        "id": "vtqxpX8pauzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WindowSeqDataset(Dataset):\n",
        "    def __init__(self, values: np.ndarray, window: int, step: int = 1):\n",
        "        self.values = np.asarray(values, dtype=float)\n",
        "        self.window = window\n",
        "        self.step = step\n",
        "        self.windows = sliding_windows(self.values, window=window, step=step).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.windows.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.windows[idx]\n",
        "        x = x.reshape(-1, 1)\n",
        "        return torch.from_numpy(x)\n",
        "\n",
        "\n",
        "class LSTMAE(nn.Module):\n",
        "    def __init__(self, input_dim=1, hidden_dim=64, latent_dim=32):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim,\n",
        "                               num_layers=1, batch_first=True)\n",
        "        self.enc_fc = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "        self.decoder = nn.LSTM(input_size=latent_dim, hidden_size=hidden_dim,\n",
        "                               num_layers=1, batch_first=True)\n",
        "        self.dec_fc = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        enc_out, (h_n, c_n) = self.encoder(x)\n",
        "        h_last = enc_out[:, -1, :]\n",
        "        z = self.enc_fc(h_last)\n",
        "        z_seq = z.unsqueeze(1).repeat(1, seq_len, 1)\n",
        "\n",
        "        dec_out, _ = self.decoder(z_seq)\n",
        "        recon = self.dec_fc(dec_out)\n",
        "        return recon\n",
        "\n",
        "\n",
        "def lstm_ad_detector(values: np.ndarray, window: int = 64, step: int = 1,\n",
        "                     train_ratio: float = 0.2, epochs: int = 30, batch_size: int = 64,\n",
        "                     lr: float = 1e-3):\n",
        "\n",
        "    vals = np.asarray(values, dtype=float)\n",
        "    scaler_vals = MinMaxScaler()\n",
        "    vals_scaled = scaler_vals.fit_transform(vals.reshape(-1, 1)).ravel()\n",
        "\n",
        "    dataset_all = WindowSeqDataset(vals_scaled, window=window, step=step)\n",
        "    n_windows = len(dataset_all)\n",
        "    n_train = max(10, int(train_ratio * n_windows))\n",
        "    train_dataset = torch.utils.data.Subset(dataset_all, range(n_train))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "    model = LSTMAE(input_dim=1, hidden_dim=64, latent_dim=32).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0.0\n",
        "        for batch in train_loader:\n",
        "            batch = batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            recon = model(batch)\n",
        "            loss = criterion(recon, batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item() * batch.size(0)\n",
        "        epoch_loss /= len(train_loader.dataset)\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f\"[LSTM-AD] Epoch {epoch+1}/{epochs} - loss={epoch_loss:.6f}\")\n",
        "    model.eval()\n",
        "    all_loader = DataLoader(dataset_all, batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "    window_scores = []\n",
        "    with torch.no_grad():\n",
        "        for batch in all_loader:\n",
        "            batch = batch.to(device)\n",
        "            recon = model(batch)\n",
        "            mse = torch.mean((recon - batch) ** 2, dim=(1, 2))\n",
        "            window_scores.append(mse.cpu().numpy())\n",
        "    window_scores = np.concatenate(window_scores, axis=0)\n",
        "\n",
        "    point_scores = map_window_scores_to_points(window_scores, len(vals_scaled), window=window, step=step)\n",
        "    scaler_score = MinMaxScaler()\n",
        "    point_scores = scaler_score.fit_transform(point_scores.reshape(-1, 1)).ravel()\n",
        "    return point_scores"
      ],
      "metadata": {
        "id": "rwUDiFINakaQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Training***"
      ],
      "metadata": {
        "id": "GJrhEfuXa1e0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 64\n",
        "step = 1\n",
        "\n",
        "print(\"=== Running Series2Graph ===\")\n",
        "scores_s2g = series2graph_detector(values, window=window_size, step=step)\n",
        "metrics_s2g = evaluate(scores_s2g, labels)\n",
        "print(\"Series2Graph-style AUC-ROC = {:.4f}, AUC-PR = {:.4f}\".format(\n",
        "    metrics_s2g[\"auc_roc\"], metrics_s2g[\"auc_pr\"]\n",
        "))\n",
        "\n",
        "print(\"\\n=== Running AE-MLP ===\")\n",
        "scores_ae = autoencoder_detector(values, window=window_size, step=step,\n",
        "                                 train_ratio=0.2, epochs=30, batch_size=128, lr=1e-3)\n",
        "metrics_ae = evaluate(scores_ae, labels)\n",
        "print(\"AE-MLP AUC-ROC = {:.4f}, AUC-PR = {:.4f}\".format(\n",
        "    metrics_ae[\"auc_roc\"], metrics_ae[\"auc_pr\"]\n",
        "))\n",
        "\n",
        "print(\"\\n=== Running LSTM-AD (LSTM Autoencoder) ===\")\n",
        "scores_lstm = lstm_ad_detector(values, window=window_size, step=step,\n",
        "                               train_ratio=0.2, epochs=30, batch_size=64, lr=1e-3)\n",
        "metrics_lstm = evaluate(scores_lstm, labels)\n",
        "print(\"LSTM-AD AUC-ROC = {:.4f}, AUC-PR = {:.4f}\".format(\n",
        "    metrics_lstm[\"auc_roc\"], metrics_lstm[\"auc_pr\"]\n",
        "))\n",
        "\n",
        "print(\"\\n=== Summary ===\")\n",
        "print(\"Series2Graph-style:\", metrics_s2g)\n",
        "print(\"AE-MLP           :\", metrics_ae)\n",
        "print(\"LSTM-AD          :\", metrics_lstm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KquVG9tiawev",
        "outputId": "6822fca2-b34e-48b4-aa73-be567d90dc03"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Running Series2Graph ===\n",
            "Series2Graph-style AUC-ROC = 0.6605, AUC-PR = 0.0831\n",
            "\n",
            "=== Running AE-MLP ===\n",
            "[AE] Epoch 5/30 - loss=180180.053199\n",
            "[AE] Epoch 10/30 - loss=149565.944901\n",
            "[AE] Epoch 15/30 - loss=137121.709265\n",
            "[AE] Epoch 20/30 - loss=128225.562185\n",
            "[AE] Epoch 25/30 - loss=121873.575175\n",
            "[AE] Epoch 30/30 - loss=117500.721613\n",
            "AE-MLP AUC-ROC = 0.5288, AUC-PR = 0.0609\n",
            "\n",
            "=== Running LSTM-AD (LSTM Autoencoder) ===\n",
            "[LSTM-AD] Epoch 5/30 - loss=0.004293\n",
            "[LSTM-AD] Epoch 10/30 - loss=0.004277\n",
            "[LSTM-AD] Epoch 15/30 - loss=0.004279\n",
            "[LSTM-AD] Epoch 20/30 - loss=0.004274\n",
            "[LSTM-AD] Epoch 25/30 - loss=0.004267\n",
            "[LSTM-AD] Epoch 30/30 - loss=0.004253\n",
            "LSTM-AD AUC-ROC = 0.3870, AUC-PR = 0.0407\n",
            "\n",
            "=== Summary ===\n",
            "Series2Graph-style: {'auc_roc': np.float64(0.660534837070178), 'auc_pr': np.float64(0.08306349599601073)}\n",
            "AE-MLP           : {'auc_roc': np.float64(0.5288487006678851), 'auc_pr': np.float64(0.06088029043583067)}\n",
            "LSTM-AD          : {'auc_roc': np.float64(0.38697062842704655), 'auc_pr': np.float64(0.040727452326283435)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 實驗結果分析\n",
        "\n",
        "本次作業使用 TSB-UAD 資料集，比較 **Series2Graph** (圖形方法) 與深度學習模型 (**AE-MLP**, **LSTM-AD**) 的異常偵測效能。\n",
        "\n",
        "#### 1. 效能比較表\n",
        "\n",
        "| 模型 (Model) | 方法類型 | AUC-ROC | AUC-PR | 排名 |\n",
        "| :--- | :--- | :--- | :--- | :--- |\n",
        "| **Series2Graph** | **Graph-based** | **0.6605** | **0.0831** | **1** |\n",
        "| AE-MLP | Reconstruction | 0.5288 | 0.0609 | 2 |\n",
        "| LSTM-AD | Forecasting/Recon | 0.3870 | 0.0407 | 3 |\n",
        "\n",
        "#### 2. 結果觀察\n",
        "\n",
        "* **Series2Graph 表現最佳**：\n",
        "    AUC-ROC 達 **0.66**，顯著優於其他模型。這印證了簡報所述，透過圖形節點 (Nodes) 捕捉重複的子序列形狀 (Shape patterns)，能更穩健地辨識出異常路徑，且不依賴大量的訓練參數。\n",
        "\n",
        "* **深度學習模型表現不佳**：\n",
        "    * **AE-MLP (0.53)**：僅略高於隨機猜測，這顯示單純利用全連接層 (MLP) 進行壓縮與重建，在捕捉此資料集的時間序列特徵上效果有限，導致正常數據與異常數據的重建誤差差異不夠明顯。\n",
        "    * **LSTM-AD (0.39)**：表現最差 (AUC < 0.5)，這通常代表模型受到訓練資料中雜訊的影響，又或是產生過擬合 (Overfitting)等情況，導致模型連「異常數據」都能準確重建，使得異常分數無法突顯，無法有效利用重建誤差來偵測異常。\n",
        "\n",
        "#### 3. 結論\n",
        "實驗結果符合 Paparrizos 等人 (2025) 的觀察及研究結論：\n",
        "* **複雜的深度學習模型不一定優於傳統或圖形方法** 。儘管 LSTM 和 AE 是熱門的深度學習方法，但在特定的時間序列基準測試中， Series2Graph 在非監督式且無參數調整的情況下，展現了較佳的魯棒性跟效能。\n",
        "* **方法特性的差異**： Series2Graph 不需要假設資料分佈，且是非監督式學習，這使其在面對未知型態的異常時更具優勢 ；而重建類方法 (AE, LSTM-AD) 則高度依賴於「正常資料能被良好重建，而異常資料不能」的假設，這在複雜的真實數據中不一定總是成立。"
      ],
      "metadata": {
        "id": "I3fAH9Dnnj2j"
      }
    }
  ]
}